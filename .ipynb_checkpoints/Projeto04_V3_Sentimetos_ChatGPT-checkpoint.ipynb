{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color='blue'>Data Science Academy - Formação Cientista de Dados</font>\n",
    "# <font color='blue'>Autor: Evandro Eulálio Cleto</font>\n",
    "\n",
    "## <font color='blue'>Data Início: 07/06/2023</font>\n",
    "## <font color='blue'>Data Finalização: 13/06/2023</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](imagens/Projeto_imagem.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='blue'>Objetivo deste projeto:</font>\n",
    "### <font color='blue'>Através da análise de Tweets sobre o ChatGPT foi construído um processo de análise que permite identificar o sentimento que predomina, especialmente no Twitter, sobre o ChatGPT.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resumo do Projeto: Criar um projeto de previsão de sentimentos sobre ChatGPT atráves de Tweets on-line usando Machine Learning.\n",
    "Os sentimentos serão previstos como positivo, negativo ou neutro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acesse http://localhost:4040 para acompanhar a execução dos jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Streaming - Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalação de pacotes necessários para o projeto\n",
    "#!pip install requests_oauthlib\n",
    "#!pip install twython\n",
    "#!pip install nltk\n",
    "#!pip install emoji\n",
    "#!pip install imblearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://pypi.org/project/findspark/\n",
    "!pip install -q findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o findspark e inicializa\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Módulos usados\n",
    "from pyspark.streaming import StreamingContext\n",
    "#from pyspark.streaming.twitter import TwitterUtils\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from requests_oauthlib import OAuth1Session\n",
    "from operator import add\n",
    "import requests_oauthlib\n",
    "from time import gmtime, strftime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "import string\n",
    "import ast\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pacote NLTK\n",
    "import nltk\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baixa Stopwords do pacote NLTK\n",
    "# https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa o arquivo CSV como DataFrame do pandas\n",
    "# Esse é um dataset com 219294 registros de tweets do Chat GPT rotulado com sentimentos positivo, negativo que \n",
    "# será utilizado para treinamento do NaiveBayesClassifier.\n",
    "# O dataset foi obtido em:  https://www.kaggle.com/datasets/charunisa/chatgpt-sentiment-analysis\n",
    "df = pd.read_csv(\"dados/ChatGPT_sentiment_orig.csv\",sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o tipo do objeto\n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shape dos dados\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove a coluna 'code' pois não tem relevância para o projeto\n",
    "df = df.drop('code', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Altera posição da coluna 'labels' para a posição 0\n",
    "cols = df.columns.tolist()\n",
    "cols = ['labels'] + cols[:cols.index('labels')] + cols[cols.index('labels')+1:]\n",
    "df = df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove vírgulas, exceto as do final da linha, da coluna 'tweets' evitar problemas na função que remove pontuação\n",
    "df['tweets'] = df['tweets'].apply(lambda x: re.sub(r'(?<!\\n),', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapear as classes para valores numéricos\n",
    "label_mapping = {'bad': 0, 'good': 1, 'neutral': 2}\n",
    "df['labels'] = df['labels'].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover emojis e caracteres especiais da coluna 'tweets'\n",
    "df['tweets'] = df['tweets'].apply(lambda x: re.sub(r'[^\\w\\s,]', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Função para retirar o conteúdo  HTTP da coluna  tweets\n",
    "def remove_urls(text):\n",
    "    # Padrão para identificar URLs com \"http\" seguido por não espaços\n",
    "    pattern = r\"http\\S+\"\n",
    "    # Substituir o padrão pela string vazia\n",
    "    clean_text = re.sub(pattern, \"\", text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função à coluna \"tweets\" do DataFrame\n",
    "df[\"tweets\"] = df[\"tweets\"].apply(remove_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratamento do dataset para filtro dos tweets no idíoma inglês"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtém as stopwords em todos os idiomas\n",
    "dicionario_stopwords = {lang: set(nltk.corpus.stopwords.words(lang)) for lang in nltk.corpus.stopwords.fileids()}\n",
    "\n",
    "dicionario_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para detectar o idioma predominante com base nas stopswords\n",
    "def descobre_idioma(text):\n",
    "    \n",
    "    #Aplica tokenização considerando pontuação\n",
    "    palavras = set(nltk.wordpunct_tokenize(text.lower()))\n",
    "    \n",
    "    # Conta o total de palavras tokenizadas considerando o dicionario de stopwords\n",
    "    lang = max(((lang, len(palavras & stopwords)) for lang, stopwords in dicionario_stopwords.items()), key = lambda x: x[1])[0]\n",
    "    \n",
    "    # Verifica se o idioma é Inglês\n",
    "    if lang == 'english':\n",
    "        return True\n",
    "    else:\n",
    "        return False    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtra somente os comentários em Inglês\n",
    "df_ingles = df[df['tweets'].apply(descobre_idioma)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shape dos dados\n",
    "df_ingles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para remover caracteres não latinos pois existem tweets que apesar de estar em Ingles possuem caratcteres \n",
    "# de linguas como Chinês, Japonês, Russo, etc\n",
    "def remove_non_latin_chars(text):\n",
    "    # Padrão para identificar caracteres não latinos\n",
    "    pattern = r'[^\\x00-\\x7F]+'\n",
    "    # Substituir o padrão pela string vazia\n",
    "    clean_text = re.sub(pattern, '', text)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função à coluna \"tweets\" do DataFrame\n",
    "df_ingles[\"tweets\"] = df_ingles[\"tweets\"].apply(remove_non_latin_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificado que exisate linhas com conteúdo ChatGPT que são propagandas de site de pornografia e serão removidos\n",
    "# Função para remover linhas com a palavra \"nMaandamano twitterfiles\"\n",
    "def remove_lines_with_keywords(text):\n",
    "    keywords = [\"nMaandamano twitterfiles\", \"AryanKhan ChatGPT\",\"Products launched\"]\n",
    "    for keyword in keywords:\n",
    "        if keyword in text:\n",
    "            return \"\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função à coluna \"tweets\" do DataFrame\n",
    "df_ingles[\"tweets\"] = df_ingles[\"tweets\"].apply(remove_lines_with_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover linhas vazias resultantes\n",
    "df_ingles = df_ingles[df_ingles[\"tweets\"] != \"\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resetar o índice do DataFrame\n",
    "df_ingles.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ingles.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shape dos dados\n",
    "df_ingles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para su bstituir \"nn\", \"\\n\" ou \"n\" seguido de número no início ou no fim de uma palavra pela string vazia\n",
    "def remove_special_chars(text):\n",
    "    # Substituir \"nn\", \"\\n\", \"n\" seguido de número ou \"nn\" seguido de número no início ou no fim de uma palavra pela string vazia\n",
    "    cleaned_text = re.sub(r'\\b(nn|\\n|n\\d+|nn\\d+)\\b', '', text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a função à coluna \"tweets\" do DataFrame\n",
    "df_ingles[\"tweets\"] = df_ingles[\"tweets\"].apply(remove_special_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shape dos dados\n",
    "df_ingles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removendo duplicidade\n",
    "df_ingles.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando o shape dos dados\n",
    "df_ingles.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva o dataframe tratado em csv\n",
    "df_ingles.to_csv('dados/ChatGPT_sentiment_limpo.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequência de update\n",
    "INTERVALO_BATCH = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria o Spark Context\n",
    "spark = SparkSession.builder.appName(\"TwitterSentimentAnalysis\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Criando o StreamingContext\n",
    "ssc = StreamingContext(sc, INTERVALO_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando o Classificador de Análise de Sentimento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O dataset limpo e filtrado pelo idioma Inglês contém 44785 tweets classificados e cada linha é marcada como: \n",
    "\n",
    "### 0 para o sentimento negativo \n",
    "### 1 para o sentimento positivo \n",
    "### 2 para o sentimento neutro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lendo o arquivo texto e criando um RDD em memória com Spark\n",
    "arquivo = sc.textFile(\"dados/ChatGPT_sentiment_limpo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arquivo.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Removendo o cabeçalho\n",
    "header = arquivo.take(1)[0]\n",
    "dataset = arquivo.filter(lambda line: line!=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Essa função separa as colunas em cada linha, cria uma tupla e remove a pontuação\n",
    "def get_row(line):\n",
    "    row = line.split(',')\n",
    "    sentimento = row[0]\n",
    "    tweet = row[1].strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "        tweet_lower.append(word.lower())\n",
    "    return(tweet_lower,sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aplica a função a cada linha do dataset\n",
    "dataset_treino = dataset.map(lambda line: get_row(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um objeto SentimentAnalyser\n",
    "sentiment_analyser = SentimentAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtem a lista de stopwords\n",
    "stopwords_all = []\n",
    "for word in stopwords.words('english'):\n",
    "    stopwords_all.append(word)\n",
    "    stopwords_all.append(word + '_NEG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtem 31.350(70%) tweets do dataset de treino e retorna todas as palavras que não são Stpwords\n",
    "dataset_treino_amostra = dataset_treino.take(4200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_treino_amostra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words_neg = sentiment_analyser.all_words([mark_negation(doc) for doc in dataset_treino_amostra])\n",
    "all_words_neg_nostops = [x for x in all_words_neg if x not in stopwords_all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cria um unigram(n-grama) e extrai as features\n",
    "unigram_feats = sentiment_analyser.unigram_word_feats(all_words_neg_nostops, top_n=200)\n",
    "sentiment_analyser.add_feat_extractor(extract_unigram_feats, unigrams = unigram_feats)\n",
    "training_set = sentiment_analyser.apply_features(dataset_treino_amostra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treinar o modelo\n",
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentiment_analyser.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testa o classificador em algumas sentenças\n",
    "test_sentence1 = [(['model', 'is', 'people', 'bad'], '')]\n",
    "test_sentence2 = [(['learning', 'day', 'bit', 'work', 'today'], '')]\n",
    "test_sentence3 = [(['good', 'wonderful', 'results', 'awesome'], '')]\n",
    "test_set = sentiment_analyser.apply_features(test_sentence1)\n",
    "test_set2 = sentiment_analyser.apply_features(test_sentence2)\n",
    "test_set3 = sentiment_analyser.apply_features(test_sentence3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurando o Stream\n",
    "rdd = ssc.sparkContext.parallelize([0])\n",
    "stream = ssc.queueStream([], default=rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Total de Tweets por update\n",
    "NUM_TWEETS = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função conecta ao Twitter e retorna um número específico de Tweets (NUM_TWEETS)\n",
    "def tfunc(t, rdd):\n",
    "  return rdd.flatMap(lambda x: stream_twitter_data())\n",
    "\n",
    "def stream_twitter_data():\n",
    "   #response = requests.get(filter_url, auth = auth, stream = True)\n",
    "  response = requests.get(filter_url, auth = auth, headers=auth_header, stream=True, params=query_params)\n",
    "  print(filter_url, response)\n",
    "  count = 0\n",
    "  for line in response.iter_lines():\n",
    "    try:\n",
    "      if count > NUM_TWEETS:\n",
    "        break\n",
    "      post = json.loads(line.decode('utf-8'))\n",
    "      contents = [post['text']]\n",
    "      count += 1\n",
    "      yield str(contents)\n",
    "    except:\n",
    "      result = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = stream.transform(tfunc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_stream = stream.map(lambda line: ast.literal_eval(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função classifica os tweets, aplicando as features do modelo criado anteriormente\n",
    "def classifica_tweet(tweet):\n",
    "  sentence = [(tweet, '')]\n",
    "  test_set = sentiment_analyzer.apply_features(sentence)\n",
    "  print(tweet, classifier.classify(test_set[0][0]))\n",
    "  return(tweet, classifier.classify(test_set[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função retorna o texto do Twitter\n",
    "def get_tweet_text(rdd):\n",
    "  for line in rdd:\n",
    "    tweet = line.strip()\n",
    "    translator = str.maketrans({key: None for key in string.punctuation})\n",
    "    tweet = tweet.translate(translator)\n",
    "    tweet = tweet.split(' ')\n",
    "    tweet_lower = []\n",
    "    for word in tweet:\n",
    "      tweet_lower.append(word.lower())\n",
    "    return(classifica_tweet(tweet_lower))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria uma lista vazia para os resultados\n",
    "resultados = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essa função salva o resultado dos batches de Tweets junto com o timestamp\n",
    "def output_rdd(rdd):\n",
    "  global resultados\n",
    "  pairs = rdd.map(lambda x: (get_tweet_text(x)[1],1))\n",
    "  counts = pairs.reduceByKey(add)\n",
    "  output = []\n",
    "  for count in counts.collect():\n",
    "    output.append(count)\n",
    "  result = [time.strftime(\"%I:%M:%S\"), output]\n",
    "  resultados.append(result)\n",
    "  print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A função foreachRDD() aplica uma função a cada RDD to streaming de dados\n",
    "coord_stream.foreachRDD(lambda t, rdd: output_rdd(rdd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start streaming\n",
    "ssc.start()\n",
    "# ssc.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont = True\n",
    "while cont:\n",
    "  if len(resultados) > 5:\n",
    "    cont = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grava os resultados\n",
    "rdd_save = '/dados/r'+time.strftime(\"%I%M%S\")\n",
    "resultados_rdd = sc.parallelize(resultados)\n",
    "resultados_rdd.saveAsTextFile(rdd_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiza os resultados\n",
    "resultados_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finaliza o streaming\n",
    "ssc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
